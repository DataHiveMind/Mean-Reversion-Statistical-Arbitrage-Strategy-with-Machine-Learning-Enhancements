{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb8eb9fa",
   "metadata": {},
   "source": [
    "# Machine Learning Model Development for Statistical Arbitrage\n",
    "This notebook demonstrates the full ML workflow: feature engineering, model selection, hyperparameter optimization, robust evaluation, and model persistence, all tailored for financial time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be687cf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.12.1)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/workspaces/Mean-Reversion-Statistical-Arbitrage-Strategy-with-Machine-Learning-Enhancements/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# 1. Feature Engineering Walkthrough (using src/features.py)\n",
    "from src.features import FeatureEngineer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load your cleaned spread data (from previous notebook or file)\n",
    "spread_df = pd.read_parquet(\"../data/spread_features.parquet\")  # or generate as before\n",
    "\n",
    "fe = FeatureEngineer(spread_df)\n",
    "fe.add_zscore(\"spread\") \\\n",
    "  .add_half_life(\"spread\") \\\n",
    "  .add_hurst_exponent(\"spread\") \\\n",
    "  .add_volume_imbalance(\"buy_volume\", \"sell_volume\") \\\n",
    "  .add_bid_ask_spread(\"bid\", \"ask\") \\\n",
    "  .add_spread_autocorrelation(\"spread\") \\\n",
    "  .add_realized_volatility(\"spread\")\n",
    "\n",
    "features_df = fe.get_features()\n",
    "print(features_df.head())\n",
    "\n",
    "# Visualize feature distributions and correlations\n",
    "features_df.hist(figsize=(16, 10), bins=30)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "sns.heatmap(features_df.corr(), annot=False, cmap='coolwarm')\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a23b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Time-Series Cross-Validation & Leakage Prevention\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import numpy as np\n",
    "\n",
    "X = features_df.dropna().drop(columns=[\"target\"], errors=\"ignore\")\n",
    "y = features_df.loc[X.index, \"target\"]  # Define your target variable\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "plt.figure(figsize=(10, 2))\n",
    "for i, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "    plt.plot(train_idx, [i+1]*len(train_idx), '|', color='blue')\n",
    "    plt.plot(test_idx, [i+1]*len(test_idx), '|', color='red')\n",
    "plt.title(\"Walk-Forward TimeSeriesSplit Visualization\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"CV Fold\")\n",
    "plt.show()\n",
    "\n",
    "# (Optional) Purging and embargoing with mlfinlab\n",
    "from mlfinlab.cross_validation import PurgedKFold\n",
    "pkf = PurgedKFold(n_splits=5, embargo_td=0.01)\n",
    "# Use pkf.split(X, event_times) for advanced leakage prevention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7319e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Advanced Model Training & Tuning (XGBoost, LightGBM, CatBoost)\n",
    "from src.ml_models import MLModels\n",
    "\n",
    "ml = MLModels(model_dir=\"../models\")\n",
    "model_types = [\"xgboost\", \"lightgbm\", \"catboost\"]\n",
    "results = {}\n",
    "\n",
    "for model_type in model_types:\n",
    "    print(f\"Training {model_type}...\")\n",
    "    ml.train(X, y, model_type=model_type)\n",
    "    preds = ml.predict(X)\n",
    "    results[model_type] = preds\n",
    "\n",
    "# Bayesian Optimization with Optuna\n",
    "best_params = ml.bayesian_optimization(X, y, model_type=\"xgboost\", n_trials=20)\n",
    "print(\"Best XGBoost params:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e3f406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Robust Model Evaluation\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "for model_type, preds in results.items():\n",
    "    print(f\"Results for {model_type}:\")\n",
    "    print(classification_report(y, preds))\n",
    "    auc_score = roc_auc_score(y, preds)\n",
    "    print(f\"AUC: {auc_score:.4f}\")\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y, preds)\n",
    "    plt.plot(recall, precision, label=model_type)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c43ad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Feature Importance Analysis (SHAP)\n",
    "import shap\n",
    "\n",
    "ml.train(X, y, model_type=\"xgboost\")\n",
    "explainer = shap.Explainer(ml.model, X)\n",
    "shap_values = explainer(X)\n",
    "shap.summary_plot(shap_values, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc45f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Model Selection Criteria\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Assume 'results' is a dict: {model_type: predictions}\n",
    "# and y is the true target\n",
    "\n",
    "model_scores = {}\n",
    "for model_type, preds in results.items():\n",
    "    auc_score = roc_auc_score(y, preds)\n",
    "    model_scores[model_type] = auc_score\n",
    "    print(f\"{model_type} AUC: {auc_score:.4f}\")\n",
    "\n",
    "# Select the best model by highest AUC\n",
    "best_model_type = max(model_scores, key=model_scores.get)\n",
    "print(f\"\\nBest model by AUC: {best_model_type} (AUC={model_scores[best_model_type]:.4f})\")\n",
    "\n",
    "# Optionally, set ml.model to the best model for downstream use\n",
    "ml.train(X, y, model_type=best_model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7182ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Model Persistence & Versioning\n",
    "from src.utils import save_joblib\n",
    "\n",
    "final_model = ml.model  # Assume best model is loaded\n",
    "save_joblib(final_model, \"../models/best_model_v1.joblib\")\n",
    "print(\"Model saved for deployment and backtesting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cb6497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Scenario Analysis (at least 4 scenarios)\n",
    "scenarios = [\n",
    "    (\"Normal Market\", X),\n",
    "    (\"High Volatility\", X * (1 + np.random.normal(0, 0.05, X.shape))),\n",
    "    (\"Flash Crash\", X.copy().assign(spread=X[\"spread\"] * (1 - 0.2))),\n",
    "    (\"Regime Shift\", X.copy().assign(spread=X[\"spread\"] * (1 + np.linspace(0, 0.1, len(X)))))\n",
    "]\n",
    "\n",
    "for name, scenario_X in scenarios:\n",
    "    preds = ml.model.predict(scenario_X)\n",
    "    print(f\"Scenario: {name}\")\n",
    "    print(\"  Mean prediction:\", np.mean(preds))\n",
    "    # Add more scenario-specific evaluation as needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
